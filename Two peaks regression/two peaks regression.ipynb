{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792cdc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b172504",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the trajectories with two peaks and their corresponding labels\n",
    "xtrain2_1 = np.loadtxt('../Data/Xtrainx2_1.csv', delimiter=',')\n",
    "xtrain2_2 = np.loadtxt('../Data/Xtrainx2_2.csv', delimiter=',')\n",
    "#files with trajectories were split to be small enough to upload to GitHub, here we concatenate them\n",
    "xtrain2 = np.concatenate((xtrain2_1, xtrain2_2))\n",
    "ytrain2 = np.loadtxt('../Data/Ytrain2.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5c6dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ytrain3[:,[2,6]] selects the columns with the positions of the three peaks, located at indices 2, and 6. \n",
    "ytrain = ytrain2[:,[2,6]]\n",
    "\n",
    "#sorting the positions of the peaks for consistency\n",
    "for i in range(ytrain.shape[0]):\n",
    "    ytrain[i,:] = ytrain[i,ytrain[i,:].argsort()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4d8370",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale the labels using MinMaxScaler for normalisation\n",
    "scaler = MinMaxScaler()\n",
    "ytrain_scaled = scaler.fit_transform(ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1716a1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to split the data into training, validation and test sets, and calculate their Fourier coefficients along\n",
    "#with the corresponding labels.\n",
    "def fouriertrainvaltest(X, Y, Ntrain, Nval, Ntest):\n",
    "\n",
    "    #Generating a training set with Ntrain trajectories, a validation with Nval trajectories and a test set with\n",
    "    #Ntest trajectories. The original trajectories contain 800 time steps but we only use 400 of them, we thus take\n",
    "    #every second point\n",
    "    Xtrain = X[0:Ntrain, 0:800:2]\n",
    "    Xval = X[Ntrain:Ntrain+Nval, 0:800:2]\n",
    "    Xtest = X[Ntrain+Nval:Ntrain+Nval+Ntest, 0:800:2]\n",
    "\n",
    "    #extract the corresponding labels for the training, validation and test sets.\n",
    "    Ytrain = Y[0:Ntrain, :]\n",
    "    Yval = Y[Ntrain:Ntrain+Nval, :]\n",
    "    Ytest = Y[Ntrain+Nval:Ntrain+Nval+Ntest, :]\n",
    "    \n",
    "    #calculating the Fourier coefficients for each subset.\n",
    "    XtrainF = np.fft.fft(Xtrain)\n",
    "    XvalF = np.fft.fft(Xval)\n",
    "    XtestF = np.fft.fft(Xtest)\n",
    "\n",
    "    #Prepare to split the Fourier coefficients into their real and imaginary components. Each complex number will \n",
    "    #occupy two columns: one for the real part and one for the imaginary part. Therefore, we create new arrays that \n",
    "    #have twice the number of columns. \n",
    "    xtrain = np.zeros((XtrainF.shape[0], 2*XtrainF.shape[1]))\n",
    "    xval = np.zeros((XvalF.shape[0], 2*XvalF.shape[1]))\n",
    "    xtest = np.zeros((XtestF.shape[0], 2*XtestF.shape[1]))\n",
    "\n",
    "    #For each Fourier coefficient in the training set, split into real and imaginary parts. These parts are then\n",
    "    #stored alternately (even indices for real, odd indices for imaginary).\n",
    "    for i in range(XtrainF.shape[0]):\n",
    "        for j in range(XtrainF.shape[1]):\n",
    "            xtrain[i, 2*j] = XtrainF[i,j].real\n",
    "            xtrain[i, 2*j + 1] = XtrainF[i,j].imag\n",
    "\n",
    "    #Do the same for the test set, splitting the Fourier coefficients into their real and imaginary parts.\n",
    "    for i in range(XtestF.shape[0]):\n",
    "        for j in range(XtestF.shape[1]):\n",
    "            xtest[i, 2*j] = XtestF[i,j].real\n",
    "            xtest[i, 2*j + 1] = XtestF[i,j].imag\n",
    "\n",
    "    #Similarly, split the Fourier coefficients for the validation set.\n",
    "    for i in range(XvalF.shape[0]):\n",
    "        for j in range(XvalF.shape[1]):\n",
    "            xval[i, 2*j] = XvalF[i,j].real\n",
    "            xval[i, 2*j + 1] = XvalF[i,j].imag\n",
    "\n",
    "    #Return the transformed training, validation and test sets along with their corresponding labels\n",
    "    return(xtrain, xval, xtest, Ytrain, Yval, Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dbb2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to calculate the R-squared metric. This function takes the true and predicted values, and calculates the \n",
    "#R-squared value\n",
    "def r_square(y_true, y_pred):\n",
    "    ss_res = tf.reduce_sum(tf.square(y_true - y_pred))\n",
    "    ss_tot = tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true)))\n",
    "    return(1 - ss_res/ss_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2305c855",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating a training, validation and test set so the dimensions can be used to load the hyperparameters\n",
    "xtrainf, xval, xtest, ytrainf, yval, ytest = fouriertrainvaltest(xtrain2, ytrain_scaled, 4800, 600, 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c1c9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a HyperModel class for the Keras Tuner to optimise the architecture and hyperparameters of the model.\n",
    "class HyperModel(kt.HyperModel):\n",
    "\n",
    "    #function to build the model with hyperparmaeter tuning\n",
    "    def build(self, hp):\n",
    "        #create a sequential model\n",
    "        model = tf.keras.Sequential()\n",
    "\n",
    "        #Tune the number of neurons in the first dense layer between 32 and 512, with a step of 32. 'units_0' is the \n",
    "        #hyperparameter name, and it will be varied during tuning.\n",
    "        model.add(tf.keras.layers.Dense(\n",
    "            units=hp.Int('units_0', min_value = 32, max_value = 512, step=32),\n",
    "            input_dim = (xtrainf.shape[1]),\n",
    "            activation='relu'))\n",
    "\n",
    "        #Tune the number of additional hidden layers between 0 and 10. For each layer, tune the number of neurons\n",
    "        #between 32 and 512.\n",
    "        for i in range(hp.Int('layers', 0, 10)):\n",
    "            model.add(tf.keras.layers.Dense(\n",
    "                units=hp.Int('units_' + str(i + 1), min_value=32, max_value=512, step=32),\n",
    "                activation='relu'))\n",
    "        \n",
    "        #Add the output layer with 2 neurons (corresponding to the two peak positions in the regression task). Use \n",
    "        #the linear activation function for regression outputs\n",
    "        model.add(tf.keras.layers.Dense(2,\n",
    "                activation='linear'))\n",
    "\n",
    "\n",
    "        #Tune the learning rate of the Adam optimiser, choosing from [0.01, 0,001, 0.0001, 0.00001, 0.000001]\n",
    "        hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4, 1e-5, 1e-6])\n",
    "        \n",
    "        #Compile the model with the Adam optimiser, mean squared error loss, and r-squared metric\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = hp_learning_rate),\n",
    "                      loss=\"mean_squared_error\",\n",
    "                      metrics=[r_square])\n",
    "        \n",
    "        #return the constructed model\n",
    "        return(model)\n",
    "\n",
    "    #function to fit the model, allowing the batch size to be tuned as well\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            #Tune the batch size by selecting from [16, 32, 61, half of the training set, or the full training set]\n",
    "            batch_size=hp.Choice(\"batch_size\", [16, 32, 64, int(xtrainf.shape[0]/2), xtrainf.shape[0]]),\n",
    "            **kwargs,\n",
    "        )\n",
    "    \n",
    "#instantiate a bayesian optimisation tuner\n",
    "tuner = kt.BayesianOptimization(HyperModel(), #pass the HyperModel class\n",
    "                     objective='val_loss', #Objective is to minimise the validation loss\n",
    "                     max_trials = 100, #Perform up to 100 trials to explore different hyperparameter combinations\n",
    "                     project_name='hp_optimisation_RCregression_twoRCsnu') #project name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d15bc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieve the best hyperparameters from the search process\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=100)[0] #Get the top hyperparameter combination from 100 trials\n",
    "\n",
    "#print statements to display the optimal hyperparameters\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
    "layer is {best_hps.get('units_0')}\"\"\")\n",
    "\n",
    "#print the optimal number of hidden layers\n",
    "print(f\"\"\" The optimal number of hidden layers is {best_hps.get('layers')}\"\"\")\n",
    "\n",
    "#loop through and print the optimal number of units for each hidden layer\n",
    "for i in range(best_hps.get('layers')):\n",
    "    print(f\"\"\" The optimal number of units in layer {i + 1} is {best_hps.get('units_' + str(i + 1))}\"\"\")\n",
    "\n",
    "#print the optimal learning rate and batch size\n",
    "print(f\"\"\"the optimal learning rate for the optimizer is {best_hps.get('learning_rate')} and the optimal batch size is {best_hps.get('batch_size')}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15549613",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate a new model using the HyperModel class with the best hyperparameters obtained from tuning\n",
    "model = HyperModel().build(best_hps)\n",
    "\n",
    "#define the path where the model's weights will be saved\n",
    "checkpoint_path = \"training_RCregression_twopeaksnu.weights.h5\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "#Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a46cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the model on the training data with validation\n",
    "history = model.fit(xtrainf, ytrainf, epochs = 1000, validation_data = (xval, yval), batch_size = best_hps.get('batch_size'), verbose=1, callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93428176",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the training and validation loss values over epochs\n",
    "plt.plot(history.history['loss'], label='training')\n",
    "plt.plot(history.history['val_loss'], label='validation')\n",
    "plt.legend()\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.savefig('lossvepochs_RCregression_twopeaksnu.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff968cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the training and validation r-squared value over epochs\n",
    "plt.plot(history.history['r_square'], label='training')\n",
    "plt.plot(history.history['val_r_square'], label='validation')\n",
    "plt.legend()\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel(r\"$R^2$\")\n",
    "plt.savefig('r2vepochs_RCregression_twopeaksnu.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66ebeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluating the model on the test set\n",
    "model.evaluate(xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df204bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluating the model on the validation set\n",
    "model.evaluate(xval, yval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dd4bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluating the model on the training set \n",
    "model.evaluate(xtrainf, ytrainf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40faf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inverse transform the scaled outputs to get original values\n",
    "y_true = scaler.inverse_transform(ytest)\n",
    "y_pred = scaler.inverse_transform(model.predict(xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1023534c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#configure font properties for the plots\n",
    "plt.rc('font',family='Times New Roman')\n",
    "plt.rcParams.update({'font.size':13})\n",
    "plt.rcParams['font.size'] = 13\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "plt.rcParams['mathtext.fontset'] = 'custom'\n",
    "plt.rcParams['mathtext.rm'] = 'Times New Roman'\n",
    "plt.rcParams['axes.linewidth'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d59de15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a scatter plot of true values for $|nu_1$ versus predicted values\n",
    "plt.scatter(y_true[:,0], y_pred[:,0], marker = 'o', color='blue', label='Predictions')\n",
    "min_val = min(min(y_true[:,0]), min(y_pred[:,0]))\n",
    "max_val = max(max(y_true[:,0]), max(y_pred[:,0]))\n",
    "plt.plot([min_val, max_val], [min_val, max_val], label='True', color='orange', linewidth = 2.5)\n",
    "plt.xlabel(r\"$\\mathrm{\\nu_1 / \\omega_0}$\")\n",
    "plt.ylabel(r\"$\\mathrm{\\hat{\\nu}_1 / \\omega_0}$\")\n",
    "plt.legend()\n",
    "plt.savefig('twopeakspredictednu1vreal.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b991134a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a scatter plot of true values for $|nu_2$ versus predicted values\n",
    "plt.scatter(y_true[:,1], y_pred[:,1], marker = 'o', color='blue', label='Predictions')\n",
    "min_val = min(min(y_true[:,1]), min(y_pred[:,1]))\n",
    "max_val = max(max(y_true[:,1]), max(y_pred[:,1]))\n",
    "plt.plot([min_val, max_val], [min_val, max_val], label='True', color='orange', linewidth = 2.5)\n",
    "plt.xlabel(r\"$\\mathrm{\\nu_2 / \\omega_0}$\")\n",
    "plt.ylabel(r\"$\\mathrm{\\hat{\\nu}_2 / \\omega_0}$\")\n",
    "plt.legend()\n",
    "plt.savefig('twopeakspredictednu2vreal.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37acceb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define an array where each element is the difference between the predicted value and true value for $|nu_1$ for a \n",
    "#given trajectory\n",
    "diffs1 = y_pred[:,0] - y_true[:,0]\n",
    "#print the minimum difference to find the smallest prediction error\n",
    "print(np.amin(diffs1))\n",
    "#print the maximum difference to find the largest prediction error\n",
    "print(np.amax(diffs1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8920ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define intervals for analysing the differences between predicted and true values for $|nu_1$\n",
    "intervals1 = [-0.06, -0.0575, -0.055, -0.0525, -0.05, -0.0475, -0.045, -0.0425, -0.04, -0.0375, -0.035, -0.0325, -0.03, -0.0275, -0.025, -0.0225, -0.02, -0.0175, -0.015, -0.0125, -0.01, -0.0075, -0.005, -0.0025, 0, 0.0025, 0.005, 0.0075, 0.01, 0.0125, 0.015, 0.0175, 0.02, 0.0225, 0.025, 0.0275, 0.03, 0.0325, 0.035, 0.0375, 0.04, 0.0425, 0.045, 0.0475, 0.05, 0.0525, 0.055, 0.0575, 0.06, 0.0625, 0.065, 0.0675, 0.07, 0.0725, 0.075, 0.0775, 0.08]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f138ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialise list to store the frequencies of differences in intervals for $|nu_1$\n",
    "freq1 = []\n",
    "\n",
    "#loop through each interval\n",
    "for i in range(len(intervals1)-1):\n",
    "    #create a mask to find differences within the current interval\n",
    "    mask1 = (diffs1 >= intervals1[i]) & (diffs1 < intervals1[i+1])\n",
    "    #append the count of differences within the current interval to the frequency list\n",
    "    freq1.append(len(diffs1[mask1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db21c469",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate x positions for bars and heights\n",
    "x1 = intervals1[1:] \n",
    "heights1 = (np.array(freq1)/600)*100\n",
    "\n",
    "#create the bar chart\n",
    "plt.bar(x1, heights1, width=-0.0025, align='edge', alpha = 1, color = 'orange', edgecolor='black')  \n",
    "plt.xlabel(r\"$\\mathrm{(\\hat{\\nu}_1 - \\nu_1)/\\omega_0}$\")\n",
    "plt.ylabel('% of trajectories')\n",
    "plt.savefig('twopeaksbarchartnu1.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e45956",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define an array where each element is the difference between the predicted value and true value for $|nu_2$ for a \n",
    "#given trajectory\n",
    "diffs2 = y_pred[:,1] - y_true[:,1]\n",
    "\n",
    "#print the minimum difference to find the smallest prediction error\n",
    "print(np.amin(diffs2))\n",
    "\n",
    "#print the maximum difference to find the largest prediction error\n",
    "print(np.amax(diffs2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcc8b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define intervals for analysing the differences between predicted and true values for $|nu_2$\n",
    "intervals2 = [-0.09, -0.0875, -0.085, -0.0825, -0.08, -0.0775, -0.075, -0.0725, -0.07, -0.0675, -0.065, -0.0625, -0.06, -0.0575, -0.055, -0.0525, -0.05, -0.0475, -0.045, -0.0425, -0.04, -0.0375, -0.035, -0.0325, -0.03, -0.0275, -0.025, -0.0225, -0.02, -0.0175, -0.015, -0.0125, -0.01, -0.0075, -0.005, -0.0025, 0, 0.0025, 0.005, 0.0075, 0.01, 0.0125, 0.015, 0.0175, 0.02, 0.0225, 0.025, 0.0275, 0.03, 0.0325, 0.035, 0.0375, 0.04, 0.0425, 0.045, 0.0475, 0.05, 0.0525, 0.055, 0.0575, 0.06, 0.0625, 0.065, 0.0675, 0.07, 0.0725]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1944df91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialise list to store the frequencies of differences in intervals for $|nu_2$\n",
    "freq2 = []\n",
    "\n",
    "#loop through each interval\n",
    "for i in range(len(intervals2)-1):\n",
    "    #create a mask to find differences within the current interval\n",
    "    mask2 = (diffs2 >= intervals2[i]) & (diffs2 < intervals2[i+1])\n",
    "    #append the count of differences within the current interval to the frequency list\n",
    "    freq2.append(len(diffs2[mask2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0a08ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate x positions for bars and heights\n",
    "x2 = intervals2[1:] \n",
    "heights2 = (np.array(freq2)/600)*100\n",
    "\n",
    "#create the bar chart\n",
    "plt.bar(x2, heights2, width=-0.0025, align='edge', alpha = 1, color = 'orange', edgecolor='black') \n",
    "plt.xlabel(r\"$\\mathrm{(\\hat{\\nu}_2 - \\nu_2)/\\omega_0}$\")\n",
    "plt.ylabel('% of trajectories')\n",
    "plt.savefig('twopeaksbarchartnu2.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

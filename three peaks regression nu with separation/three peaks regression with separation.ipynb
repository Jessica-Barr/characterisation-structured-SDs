{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ea1421",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e387c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the trajectories with three peaks and their corresponding labels\n",
    "xtrainx3_1 = np.loadtxt('../Data/Xtrainx3_1.csv', delimiter=',')\n",
    "xtrainx3_2 = np.loadtxt('../Data/Xtrainx3_2.csv', delimiter=',')\n",
    "#files with trajectories were split to be small enough to upload to GitHub, here we concatenate them\n",
    "xtrainx3 = np.concatenate((xtrainx3_1, xtrainx3_2))\n",
    "ytrain3 = np.loadtxt('../Data/Ytrain3.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68172f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate an index column to the original data to keep track of the original row positions (trajectories). This\n",
    "#is important because after operations like filtering, shuffling, and training, we may lose track of the original \n",
    "#correspondence between each trajectory and its original position in the dataset. By appending the row indices as a\n",
    "#new column, we can still identify each trajectory later for further analysis.\n",
    "xtrainx3_index = np.concatenate((xtrainx3, np.arange(xtrainx3.shape[0]).reshape(xtrainx3.shape[0], 1)), axis=1)\n",
    "\n",
    "#ytrain3[:,[2,6,10]] selects the columns with the positions of the three peaks, located at indices 2, 6, and 10. \n",
    "nu3 = ytrain3[:,[2,6,10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7ce3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to split the data into training, validation and test sets, and calculate their Fourier coefficients along\n",
    "#with the corresponding labels. The function also appends the original index of each trajectory for tracking.\n",
    "def fouriertrainvaltest(X, Y, Ntrain, Nval, Ntest):\n",
    "    \n",
    "    #Generating a training set with Ntrain trajectories, a validation with Nval trajectories and a test set with\n",
    "    #Ntest trajectories. The original trajectories contain 800 time steps but we only use 400 of them, we thus take\n",
    "    #every second point\n",
    "    Xtrain = X[0:Ntrain, 0:800:2]\n",
    "    Xval = X[Ntrain:Ntrain+Nval, 0:800:2]\n",
    "    Xtest = X[Ntrain+Nval:Ntrain+Nval+Ntest, 0:800:2]\n",
    "\n",
    "    #extract the corresponding labels for the training, validation and test sets.\n",
    "    Ytrain = Y[0:Ntrain, :]\n",
    "    Yval = Y[Ntrain:Ntrain+Nval, :]\n",
    "    Ytest = Y[Ntrain+Nval:Ntrain+Nval+Ntest, :]\n",
    "\n",
    "    #calculating the Fourier coefficients for each subset.\n",
    "    XtrainF = np.fft.fft(Xtrain)\n",
    "    XvalF = np.fft.fft(Xval)\n",
    "    XtestF = np.fft.fft(Xtest)\n",
    "\n",
    "    #Prepare to split the Fourier coefficients into their real and imaginary components. Each complex number will \n",
    "    #occupy two columns: one for the real part and one for the imaginary part. Therefore, we create new arrays that \n",
    "    #have twice the number of columns. \n",
    "    xtrain = np.zeros((XtrainF.shape[0], 2*XtrainF.shape[1]))\n",
    "    xval = np.zeros((XvalF.shape[0], 2*XvalF.shape[1]))\n",
    "    xtest = np.zeros((XtestF.shape[0], 2*XtestF.shape[1]))\n",
    "\n",
    "    #For each Fourier coefficient in the training set, split into real and imaginary parts. These parts are then\n",
    "    #stored alternately (even indices for real, odd indices for imaginary).\n",
    "    for i in range(XtrainF.shape[0]):\n",
    "        for j in range(XtrainF.shape[1]):\n",
    "            xtrain[i, 2*j] = XtrainF[i,j].real\n",
    "            xtrain[i, 2*j + 1] = XtrainF[i,j].imag\n",
    "\n",
    "    #Do the same for the test set, splitting the Fourier coefficients into their real and imaginary parts.\n",
    "    for i in range(XtestF.shape[0]):\n",
    "        for j in range(XtestF.shape[1]):\n",
    "            xtest[i, 2*j] = XtestF[i,j].real\n",
    "            xtest[i, 2*j + 1] = XtestF[i,j].imag\n",
    "\n",
    "    #Similarly, split the Fourier coefficients for the validation set.\n",
    "    for i in range(XvalF.shape[0]):\n",
    "        for j in range(XvalF.shape[1]):\n",
    "            xval[i, 2*j] = XvalF[i,j].real\n",
    "            xval[i, 2*j + 1] = XvalF[i,j].imag\n",
    "        \n",
    "    #concatenating the original index from X as a new column to keep track of the trajectories. This index column\n",
    "    #allows you to track each trajectory after operations like filtering and shuffling. \n",
    "    xtrain = np.concatenate((xtrain, X[0:Ntrain,-1].reshape(xtrain.shape[0], 1)), axis=1)\n",
    "    xval = np.concatenate((xval, X[Ntrain:Ntrain+Nval,-1].reshape(xval.shape[0], 1)), axis=1)\n",
    "    xtest = np.concatenate((xtest, X[Ntrain+Nval:Ntrain+Nval+Ntest,-1].reshape(xtest.shape[0], 1)), axis=1)\n",
    "\n",
    "    #Return the transformed training, validation and test sets along with their corresponding labels\n",
    "    return(xtrain, xval, xtest, Ytrain, Yval, Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c755439",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to calculate the R-squared metric. This function takes the true and predicted values, and calculates the \n",
    "#R-squared value\n",
    "def r_square(y_true, y_pred):\n",
    "    ss_res = tf.reduce_sum(tf.square(y_true - y_pred))\n",
    "    ss_tot = tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true)))\n",
    "    return(1 - ss_res/ss_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd27a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a HyperModel class for the Keras Tuner to optimise the architecture and hyperparameters of the model.\n",
    "class HyperModel(kt.HyperModel):\n",
    "\n",
    "    #function to build the model with hyperparameter tuning\n",
    "    def build(self, hp):\n",
    "        #create a sequential model\n",
    "        model = tf.keras.Sequential()\n",
    "\n",
    "        #Tune the number of neurons in the first dense layer between 32 and 512, with a step of 32. 'units_0' is the \n",
    "        #hyperparameter name, and it will be varied during tuning.\n",
    "        model.add(tf.keras.layers.Dense(\n",
    "            units=hp.Int('units_0', min_value = 32, max_value = 512, step=32),\n",
    "            input_dim = (xtrainf.shape[1]-1),\n",
    "            activation='relu'))\n",
    "        \n",
    "        #Tune the number of additional hidden layers between 0 and 10. For each layer, tune the number of neurons\n",
    "        #between 32 and 512.\n",
    "        for i in range(hp.Int('layers', 0, 10)):\n",
    "            model.add(tf.keras.layers.Dense(\n",
    "                units=hp.Int('units_' + str(i + 1), min_value=32, max_value=512, step=32),\n",
    "                activation='relu'))\n",
    "\n",
    "        #Add the output layer with 3 neurons (corresponding to the three peak positions in the regression task). \n",
    "        #Use the linear activation function for regression outputs\n",
    "        model.add(tf.keras.layers.Dense(3,\n",
    "                activation='linear'))\n",
    "\n",
    "\n",
    "        #Tune the learning rate of the Adam optimiser, choosing from [0.01, 0,001, 0.0001, 0.00001, 0.000001]\n",
    "        hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4, 1e-5, 1e-6])\n",
    "        \n",
    "        #Compile the model with the Adam optimiser, mean squared error loss, and r-squared metric\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = hp_learning_rate),\n",
    "                      loss=\"mean_squared_error\",\n",
    "                      metrics=[r_square])\n",
    "        \n",
    "        #return the constructed model\n",
    "        return(model)\n",
    "    \n",
    "    #function to fit the model, allowing the batch size to be tuned as well\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            #Tune the batch size by selecting from [16, 32, 61, half of the training set, or the full training set]\n",
    "            batch_size=hp.Choice(\"batch_size\", [16, 32, 64, int(xtrainf.shape[0]/2), xtrainf.shape[0]]),\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "#instantiate a bayesian optimisation tuner\n",
    "tuner = kt.BayesianOptimization(HyperModel(), #pass the HyperModel class\n",
    "                     objective='val_loss', #Objective is to minimise the validation loss\n",
    "                     max_trials = 100, #Perform up to 100 trials to explore different hyperparameter combinations\n",
    "                     project_name='hp_optimisation_RCregression_threeRCsnu') #project name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d0f17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieve the best hyperparameters from the search process\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=100)[0] #Get the top hyperparameter combination from 100 trials\n",
    "\n",
    "#print statements to display the optimal hyperparameters\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
    "layer is {best_hps.get('units_0')}.\"\"\")\n",
    "\n",
    "#print the optimal number of hidden layers\n",
    "print(f\"\"\" The optimal number of hidden layers is {best_hps.get('layers')}\"\"\")\n",
    "\n",
    "#loop through and print the optimal number of units for each hidden layer\n",
    "for i in range(best_hps.get('layers')):\n",
    "  print(f\"\"\" The optimal number of units in layer {i + 1} is {best_hps.get('units_' + str(i + 1))}\"\"\")\n",
    "\n",
    "#print the optimal learning rate and batch size\n",
    "print(f\"\"\"the optimal learning rate for the optimizer is {best_hps.get('learning_rate')} and the optimal batch size is {best_hps.get('batch_size')}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dddcb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a range of epsilon values from 0 to 0.45 with a step of 0.05\n",
    "epsilon = np.arange(0, 0.45+0.05, 0.05)\n",
    "\n",
    "#initialise arrays to store the r_square and loss metric for the training, validation and test sets. These metrics\n",
    "#will be recorded for each value of epsilon\n",
    "\n",
    "#arrays to store the training r-squared and loss\n",
    "trainingr2 = np.zeros(len(epsilon)) #r_squared for overall training set\n",
    "trainingloss = np.zeros(len(epsilon)) #loss for overall training set\n",
    "\n",
    "#arrays to store the validation r-squared and loss\n",
    "valr2 = np.zeros(len(epsilon)) #r-squared for the overall validation set\n",
    "valloss = np.zeros(len(epsilon)) #loss for the overall validation set\n",
    "\n",
    "#arrays to store the test r-squared and loss\n",
    "testr2 = np.zeros(len(epsilon)) #r-squared for overall test set\n",
    "testloss = np.zeros(len(epsilon)) #loss for the overall test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc85ba91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate over each epsilon value\n",
    "for i in tqdm(range(len(epsilon))):\n",
    "    #Initialise lists to store the filtered training data and corresponding $|nu$ values\n",
    "    Xtrainx3_filtered = []\n",
    "    nus3 = []\n",
    "\n",
    "    #filter trajectories based on the epsilon criteria\n",
    "    for j in range(xtrainx3_index.shape[0]): \n",
    "        #check if the absolute differences between $|nu$ values are greater than or equal to the current epsilon\n",
    "        if np.abs(nu3[j,0] - nu3[j,1]) >= epsilon[i] and np.abs(nu3[j,0] - nu3[j,2]) >= epsilon[i] and np.abs(nu3[j,1]-nu3[j,2]) >= epsilon[i]:\n",
    "            #append the filtered trajectory and corresponding $|nu$ values\n",
    "            Xtrainx3_filtered.append(xtrainx3_index[[j],:])\n",
    "            nus3.append(nu3[[j],:])\n",
    "\n",
    "    #concatenate the filtered trajectories and $|nu$s into arrays\n",
    "    xtrainx3_filtered_arr = np.concatenate(Xtrainx3_filtered)\n",
    "    nus3_arr = np.concatenate(nus3)\n",
    "    \n",
    "    #sort each row of nus3_arr for consistency\n",
    "    for k in range(nus3_arr.shape[0]):\n",
    "        nus3_arr[k,:] = nus3_arr[k, nus3_arr[k,:].argsort()]\n",
    "        \n",
    "    #create an array of indices for shuffling\n",
    "    indices = np.arange(xtrainx3_filtered_arr.shape[0])\n",
    "    indices_shuffle = np.random.permutation(indices)\n",
    "    \n",
    "    #shuffling the filtered training data\n",
    "    xtrain = xtrainx3_filtered_arr[indices_shuffle]\n",
    "    ytrain = nus3_arr[indices_shuffle]\n",
    "    \n",
    "    #Defining sizes for training, validation and test sets (80-10-10 split)\n",
    "    Ntrain = xtrainx3_filtered_arr.shape[0] - 2*int(xtrainx3_filtered_arr.shape[0]*0.1)\n",
    "    Nval = int(xtrainx3_filtered_arr.shape[0]*0.1)\n",
    "    Ntest = int(xtrainx3_filtered_arr.shape[0]*0.1)\n",
    "    \n",
    "    #scale the labels using MinMaxScaler for normalisation\n",
    "    scaler = MinMaxScaler()\n",
    "    ytrain_scaled = scaler.fit_transform(ytrain)\n",
    "    \n",
    "    #generate training, validation and test sets using the defined function\n",
    "    xtrainf, xval, xtest, ytrainf, yval, ytest = fouriertrainvaltest(xtrain, ytrain_scaled, Ntrain, Nval, Ntest)\n",
    "\n",
    "    #Build the model with the best hyperparameters\n",
    "    model = HyperModel().build(best_hps)\n",
    "    \n",
    "    #Fit the model on the training data with validation\n",
    "    history = model.fit(xtrainf[:,:-1], ytrainf, epochs = 1000, validation_data = (xval[:,:-1], yval), batch_size = best_hps.get('batch_size'), verbose=0)\n",
    "    \n",
    "    #save the trained model weights\n",
    "    model.save(\"Weights/training_RCregression_threepeaksnu_eilson{0}.weights.h5\".format(epsilon[i]))\n",
    "    \n",
    "    #evaluate the model on the training, validation and test sets and store the loss and r-squared metrics\n",
    "    trainingloss[i], trainingr2[i] = model.evaluate(xtrainf[:,:-1], ytrainf)\n",
    "    valloss[i], valr2[i] = model.evaluate(xval[:,:-1], yval)\n",
    "    testloss[i], testr2[i] = model.evaluate(xtest[:,:-1], ytest)\n",
    "    \n",
    "    #save the loss and r-square metrics to csv files\n",
    "    np.savetxt('traininglossvepsilon.csv', trainingloss, delimiter=',')\n",
    "    np.savetxt('trainingr2vepsilon.csv', trainingr2, delimiter=',')\n",
    "    np.savetxt('vallossvepsilon.csv', valloss, delimiter=',')\n",
    "    np.savetxt('valr2vepsilon.csv', valr2, delimiter=',')\n",
    "    np.savetxt('testlossvepsilon.csv', testloss, delimiter=',')\n",
    "    np.savetxt('testr2vepsilon.csv', testr2, delimiter=',')\n",
    "    \n",
    "    #Make predictions for the training validation, and test sets\n",
    "    predictions_train = scaler.inverse_transform(model.predict(xtrainf[:,:-1]))\n",
    "    predictions_val = scaler.inverse_transform(model.predict(xval[:,:-1]))\n",
    "    predictions_test = scaler.inverse_transform(model.predict(xtest[:,:-1]))\n",
    "    \n",
    "    #save predictions alongside the original index for training, validation and test sets\n",
    "    np.savetxt('Predictions/predictionstrain_epsilon{0}.csv'.format(epsilon[i]), np.concatenate((predictions_train, xtrainf[:,[-1]]), axis=1), delimiter=',')\n",
    "    np.savetxt('Predictions/predictionsval_epsilon{0}.csv'.format(epsilon[i]), np.concatenate((predictions_val, xval[:,[-1]]), axis=1), delimiter=',')\n",
    "    np.savetxt('Predictions/predictionstest_epsilon{0}.csv'.format(epsilon[i]), np.concatenate((predictions_test, xtest[:,[-1]]), axis=1), delimiter=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

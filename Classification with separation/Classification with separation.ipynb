{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ea1421",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "import scipy as sc\n",
    "from scipy import integrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdab050",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the trajectories and their corresponding labels\n",
    "\n",
    "#loading the trajectories with one peak\n",
    "xtrainx1_1 = np.loadtxt('../Data/Xtrainx1_1.csv', delimiter=',')\n",
    "xtrainx1_2 = np.loadtxt('../Data/Xtrainx1_2.csv', delimiter=',')\n",
    "#loading the trajectories with two peaks\n",
    "xtrainx2_1 = np.loadtxt('../Data/Xtrainx2_1.csv', delimiter=',')\n",
    "xtrainx2_2 = np.loadtxt('../Data/Xtrainx2_2.csv', delimiter=',')\n",
    "#loading the trajectories with three peaks\n",
    "xtrainx3_1 = np.loadtxt('../Data/Xtrainx3_1.csv', delimiter=',')\n",
    "xtrainx3_2 = np.loadtxt('../Data/Xtrainx3_1.csv', delimiter=',')\n",
    "\n",
    "#files with trajectories were split to be small enough to upload to GitHub, here we concatenate them\n",
    "xtrainx1 = np.concatenate((xtrainx1_1, xtrainx1_2))\n",
    "xtrainx2 = np.concatenate((xtrainx2_1, xtrainx2_2))\n",
    "xtrainx3 = np.concatenate((xtrainx3_1, xtrainx3_2))\n",
    "\n",
    "#loading the labels for the trajectories with one peak\n",
    "ytrain1 = np.loadtxt('../Data/Ytrain1.csv', delimiter=',')\n",
    "#loading the labels for the trajectories with two peaks\n",
    "ytrain2 = np.loadtxt('../Data/Ytrain2.csv', delimiter=',')\n",
    "#loading the labels for the trajectories with three peaks\n",
    "ytrain3 = np.loadtxt('../Data/Ytrain3.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e1cfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate an index column to the original data to keep track of the original row positions (trajectories). This\n",
    "#is important because after operations like filtering, shuffling, and training, we may lose track of the original \n",
    "#correspondence between each trajectory and its original position in the dataset. By appending the row indices as a\n",
    "#new column, we can still identify each trajectory later for further analysis.\n",
    "xtrainx1_index = np.concatenate((xtrainx1, np.arange(xtrainx1.shape[0]).reshape(xtrainx1.shape[0], 1)), axis=1)\n",
    "xtrainx2_index = np.concatenate((xtrainx2, np.arange(xtrainx2.shape[0]).reshape(xtrainx2.shape[0], 1)), axis=1)\n",
    "xtrainx3_index = np.concatenate((xtrainx3, np.arange(xtrainx3.shape[0]).reshape(xtrainx3.shape[0], 1)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6fd581",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the columns from ytrain2 and ytrain3 that contain the positions of the peaks\n",
    "nu2 = ytrain2[:,[2,6]]\n",
    "nu3 = ytrain3[:,[2,6,10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7ce3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to split the data into training, validation and test sets, and calculate their Fourier coefficients along\n",
    "#with the corresponding labels. The function also appends the original index of each trajectory for tracking.\n",
    "def fouriertrainvaltest(X, Y, Ntrain, Nval, Ntest):\n",
    "\n",
    "    #Generating a training set with Ntrain trajectories, a validation with Nval trajectories and a test set with\n",
    "    #Ntest trajectories. The original trajectories contain 800 time steps but we only use 400 of them, we thus take\n",
    "    #every second point\n",
    "    Xtrain = X[0:Ntrain, 0:800:2]\n",
    "    Xval = X[Ntrain:Ntrain+Nval, 0:800:2]\n",
    "    Xtest = X[Ntrain+Nval:Ntrain+Nval+Ntest, 0:800:2]\n",
    "\n",
    "    #extract the corresponding labels for the training, validation and test sets.\n",
    "    Ytrain = Y[0:Ntrain, :]\n",
    "    Yval = Y[Ntrain:Ntrain+Nval, :]\n",
    "    Ytest = Y[Ntrain+Nval:Ntrain+Nval+Ntest, :]\n",
    "\n",
    "    #calculating the Fourier coefficients for each subset.\n",
    "    XtrainF = np.fft.fft(Xtrain)\n",
    "    XvalF = np.fft.fft(Xval)\n",
    "    XtestF = np.fft.fft(Xtest)\n",
    "\n",
    "    #Prepare to split the Fourier coefficients into their real and imaginary components. Each complex number will \n",
    "    #occupy two columns: one for the real part and one for the imaginary part. Therefore, we create new arrays that \n",
    "    #have twice the number of columns. \n",
    "    xtrain = np.zeros((XtrainF.shape[0], 2*XtrainF.shape[1]))\n",
    "    xval = np.zeros((XvalF.shape[0], 2*XvalF.shape[1]))\n",
    "    xtest = np.zeros((XtestF.shape[0], 2*XtestF.shape[1]))\n",
    "\n",
    "    #For each Fourier coefficient in the training set, split into real and imaginary parts. These parts are then\n",
    "    #stored alternately (even indices for real, odd indices for imaginary).\n",
    "    for i in range(XtrainF.shape[0]):\n",
    "        for j in range(XtrainF.shape[1]):\n",
    "            xtrain[i, 2*j] = XtrainF[i,j].real\n",
    "            xtrain[i, 2*j + 1] = XtrainF[i,j].imag\n",
    "\n",
    "    #Do the same for the test set, splitting the Fourier coefficients into their real and imaginary parts.\n",
    "    for i in range(XtestF.shape[0]):\n",
    "        for j in range(XtestF.shape[1]):\n",
    "            xtest[i, 2*j] = XtestF[i,j].real\n",
    "            xtest[i, 2*j + 1] = XtestF[i,j].imag\n",
    "\n",
    "    #Similarly, split the Fourier coefficients for the validation set.\n",
    "    for i in range(XvalF.shape[0]):\n",
    "        for j in range(XvalF.shape[1]):\n",
    "            xval[i, 2*j] = XvalF[i,j].real\n",
    "            xval[i, 2*j + 1] = XvalF[i,j].imag\n",
    "            \n",
    "    #concatenating the original index from X as a new column to keep track of the trajectories. This index column\n",
    "    #allows you to track each trajectory after operations like filtering and shuffling. \n",
    "    xtrain = np.concatenate((xtrain, X[0:Ntrain,-1].reshape(xtrain.shape[0], 1)), axis=1)\n",
    "    xval = np.concatenate((xval, X[Ntrain:Ntrain+Nval,-1].reshape(xval.shape[0], 1)), axis=1)\n",
    "    xtest = np.concatenate((xtest, X[Ntrain+Nval:Ntrain+Nval+Ntest,-1].reshape(xtest.shape[0], 1)), axis=1)\n",
    "\n",
    "    #Return the transformed training, validation and test sets along with their corresponding labels\n",
    "    return(xtrain, xval, xtest, Ytrain, Yval, Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04556e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating a training, validation and test set so the dimensions can be used to load the hyperparameters\n",
    "xtrainf, xval, xtest, ytrainf, yval, ytest = fouriertrainvaltest(xtrainx3, ytrain3, 4800, 600, 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd27a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a HyperModel class for the Keras Tuner to optimise the model's architecture and hyperparameters\n",
    "class HyperModel(kt.HyperModel):\n",
    "\n",
    "    #function to build the model with hyperparameter tuning\n",
    "    def build(self, hp):\n",
    "        #create a sequential model\n",
    "        model = tf.keras.Sequential()\n",
    "\n",
    "        #Tune the number of neurons in the first dense layer between 32 and 512, with a step of 32. 'units_0' is the \n",
    "        #hyperparameter name, and it will be varied during tuning.\n",
    "        model.add(tf.keras.layers.Dense(\n",
    "            units=hp.Int('units_0', min_value = 32, max_value = 512, step=32),\n",
    "            input_dim = (xtrainf.shape[1]-1),\n",
    "            activation='relu'))\n",
    "\n",
    "        #Tune the number of additional hidden layers between 0 and 10. For each layer, tune the number of neurons\n",
    "        #between 32 and 512.\n",
    "        for i in range(hp.Int('layers', 0, 10)):\n",
    "            model.add(tf.keras.layers.Dense(\n",
    "                units=hp.Int('units_' + str(i + 1), min_value=32, max_value=512, step=32),\n",
    "                activation='relu'))\n",
    "\n",
    "        #Add the output layer with 3 neurons (for classification with three outputs), using the softmax activation \n",
    "        #function\n",
    "        model.add(tf.keras.layers.Dense(3,\n",
    "                activation='softmax'))\n",
    "\n",
    "\n",
    "        #Tune the learning rate of the Adam optimiser, choosing from [0.01, 0,001, 0.0001, 0.00001, 0.000001]\n",
    "        hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4, 1e-5, 1e-6])\n",
    "        \n",
    "        #compile the model with the Adam optimiser using the tuned learning rate, categorical cross-entropy loss,\n",
    "        #and categorical accuracy\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = hp_learning_rate),\n",
    "                      loss=\"categorical_crossentropy\",\n",
    "                      metrics = 'categorical_accuracy')\n",
    "        \n",
    "        #return the constructed model\n",
    "        return(model)\n",
    "\n",
    "    #function to fit the model, allowing the batch size to be tuned as well\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            #Tune the batch size by selecting from [16, 32, 61, half of the training set, or the full training set]\n",
    "            batch_size=hp.Choice(\"batch_size\", [16, 32, 64, int(xtrain.shape[0]/2), xtrain.shape[0]]),\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "#instantiate a bayesian optimisation tuner\n",
    "tuner = kt.BayesianOptimization(HyperModel(), #pass the HyperModel class\n",
    "                     objective='val_loss', #Objective is to minimise the validation loss\n",
    "                     max_trials = 100, #Perform up to 100 trials to explore different hyperparameter combinations\n",
    "                     project_name='hp_optimisation_RCclassification_onetwoorthreepeaks') #project name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d0f17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieve the best hyperparameters from the search process\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=100)[0] #Get the top hyperparameter combination from 100 trials\n",
    "\n",
    "#print statements to display the optimal hyperparameters\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
    "layer is {best_hps.get('units_0')}.\"\"\")\n",
    "\n",
    "#print the optimal number of hidden layers\n",
    "print(f\"\"\" The optimal number of hidden layers is {best_hps.get('layers')}\"\"\")\n",
    "\n",
    "#loop through and print the optimal number of units for each hidden layer\n",
    "for i in range(best_hps.get('layers')):\n",
    "  print(f\"\"\" The optimal number of units in layer {i + 1} is {best_hps.get('units_' + str(i + 1))}\"\"\")\n",
    "\n",
    "#print the optimal learning rate and batch size\n",
    "print(f\"\"\"the optimal learning rate for the optimizer is {best_hps.get('learning_rate')} and the optimal batch size is {best_hps.get('batch_size')}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657cf6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to categorise the model's predictions into correct and incorrect label categories\n",
    "def correctandincorrectlabels(x, y):\n",
    "    \n",
    "    #initialise empty lists to store categorised predictions based on their true labels and predicted labels\n",
    "    \n",
    "    p1l1 = [] #true label 1, predicted label 1 (correct)\n",
    "    p1l2 = [] #true label 1, predicted label 1 (incorrect)\n",
    "    p1l3 = [] #true label 1, predicted label 3 (incorrect)\n",
    "\n",
    "    p2l1 = [] #true label 2, predicted label 1 (incorrect)\n",
    "    p2l2 = [] #true label 2, predicted label 2 (correct)\n",
    "    p2l3 = [] #true label 2, predicted label 3 (incorrect)\n",
    "\n",
    "    p3l1 = [] #true label 3, predicted label 1 (incorrect)\n",
    "    p3l2 = [] #true label 3, predicted label 2 (incorrect)\n",
    "    p3l3 = [] #true label 3, predicted label 3 (correct)\n",
    "    \n",
    "    #predict labels using the trained model (ignoring the last column in x which contains the indices for tracking)\n",
    "    predictions = model.predict(x[:,:-1])\n",
    "\n",
    "    #loop through all trajectories to categorise based on true and predicted labels\n",
    "    for i in range(x.shape[0]):\n",
    "\n",
    "        #if the true label is 1 (i.e., the index with the max value in y[i,:] is 0)\n",
    "        if y[i,:].argmax() == 0:\n",
    "\n",
    "            #predicted label is also 1 (correct classification for label 1)\n",
    "            if predictions[i,:].argmax() == 0:\n",
    "                #add to list with indices for tracking\n",
    "                p1l1.append(np.concatenate((predictions[[i],:], x[i,-1].reshape(1,1)), axis=1)) \n",
    "\n",
    "            #predicted label is 2 (misclassified as label 2)\n",
    "            if predictions[i,:].argmax() == 1:\n",
    "                p1l2.append(np.concatenate((predictions[[i],:], x[i,-1].reshape(1,1)), axis=1))\n",
    "\n",
    "            #predicted label is 3 (misclassified as label 3)\n",
    "            if predictions[i,:].argmax() == 2:\n",
    "                p1l3.append(np.concatenate((predictions[[i],:], x[i,-1].reshape(1,1)), axis=1))\n",
    "\n",
    "        #if the true label is 2 (i.e, the index with the max value in y[i,:] is 1)\n",
    "        if y[i,:].argmax() == 1:\n",
    "\n",
    "            #predicted label is 1 (misclassified as label 1)\n",
    "            if predictions[i,:].argmax() == 0:\n",
    "                p2l1.append(np.concatenate((predictions[[i],:], x[i,-1].reshape(1,1)), axis=1))\n",
    "\n",
    "            #predicted label is 2 (correct classification for label 2)\n",
    "            if predictions[i,:].argmax() == 1:\n",
    "                p2l2.append(np.concatenate((predictions[[i],:], x[i,-1].reshape(1,1)), axis=1))\n",
    "\n",
    "            #predicted label is 3 (misclassified as label 3)\n",
    "            if predictions[i,:].argmax() == 2:\n",
    "                p2l3.append(np.concatenate((predictions[[i],:], x[i,-1].reshape(1,1)), axis=1))\n",
    "\n",
    "        #if the true label is 3 (i.e., the index with the max value in y[i,:] is 2)\n",
    "        if y[i,:].argmax() == 2:\n",
    "            \n",
    "            #predicted label is 1 (misclassified as label 1)\n",
    "            if predictions[i,:].argmax() == 0:\n",
    "                p3l1.append(np.concatenate((predictions[[i],:], x[i,-1].reshape(1,1)), axis=1))\n",
    "\n",
    "            #predicted label is 2 (misclassified as label 2)\n",
    "            if predictions[i,:].argmax() == 1:\n",
    "                p3l2.append(np.concatenate((predictions[[i],:], x[i,-1].reshape(1,1)), axis=1))\n",
    "\n",
    "            #predicted label is 3 (correct classification for label 3)\n",
    "            if predictions[i,:].argmax() == 2:\n",
    "                p3l3.append(np.concatenate((predictions[[i],:], x[i,-1].reshape(1,1)), axis=1))\n",
    "    \n",
    "    #return the categorised predictions (9 lists total, 3 categories for each true label)\n",
    "    return(p1l1, p1l2, p1l3, p2l1, p2l2, p2l3, p3l1, p3l2, p3l3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8729e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a range of epsilon values from 0 to 0.45 with a step of 0.05\n",
    "epsilon = np.arange(0, 0.45+0.05, 0.05)\n",
    "\n",
    "#initialise arrays to store the classification accuracy and loss metric for the training, validation and test sets.\n",
    "#These metrics will be recorded for each value of epsilon\n",
    "\n",
    "#arrays to store the training classification accuracy and loss\n",
    "trainingacc = np.zeros(len(epsilon)) #classification accuracy for training set\n",
    "trainingloss = np.zeros(len(epsilon)) #loss for training set\n",
    "\n",
    "#arrays to store the validation classification accuracy and loss\n",
    "valacc = np.zeros(len(epsilon)) #classification accuracy for validation set\n",
    "valloss = np.zeros(len(epsilon)) #loss for validation set\n",
    "\n",
    "#arrays to store the test classification accuracy and loss\n",
    "testacc = np.zeros(len(epsilon)) #classification accuracy for test set\n",
    "testloss = np.zeros(len(epsilon)) #loss for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc85ba91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate over all values of epsilon, tracking the progress with tqdm\n",
    "for i in tqdm(range(len(epsilon))):\n",
    "    \n",
    "    #initialise lists to store the filtered trajectories with 2 and 3 peaks\n",
    "    Xtrainx2_filtered = []\n",
    "    Xtrainx3_filtered = []\n",
    "\n",
    "    #loop through the trajectories with 2 peaks and filter based on the condition that the difference between the \n",
    "    #positions of the peaks is greater than or equal to the current epsilon\n",
    "    for j in range(xtrainx2_index.shape[0]):        \n",
    "        if np.abs(nu2[j,0] - nu2[j,1]) >= epsilon[i]:\n",
    "            Xtrainx2_filtered.append(xtrainx2_index[[j],:])\n",
    "\n",
    "    #loop through the trajectories with 3 peaks and filter based on the condition that all pairwise differences\n",
    "    #between the positions of the peaks are greater than or equal to the current epsilon\n",
    "    for k in range(xtrainx3_index.shape[0]):\n",
    "        if np.abs(nu3[k,0] - nu3[k,1]) >= epsilon[i] and np.abs(nu3[k,0] - nu3[k,2]) >= epsilon[i] and np.abs(nu3[k,1]-nu3[k,2]) >= epsilon[i]:\n",
    "            Xtrainx3_filtered.append(xtrainx3_index[[k],:])\n",
    "\n",
    "    #convert the filter lists into numpy arrays\n",
    "    Xtrainx2_filtered_arr = np.concatenate(Xtrainx2_filtered)\n",
    "    Xtrainx3_filtered_arr = np.concatenate(Xtrainx3_filtered)\n",
    "\n",
    "    #create one-hot encoded labels for the three classes\n",
    "    ytrain1 = np.zeros((xtrainx1_index.shape[0], 3)) #labels for trajectories with one peak\n",
    "    ytrain2 = np.zeros((Xtrainx2_filtered_arr.shape[0], 3)) #labels for trajectories with two peaks\n",
    "    ytrain3 = np.zeros((Xtrainx3_filtered_arr.shape[0], 3)) #labels for trajectories with three peaks\n",
    "\n",
    "    #assign the correct label for each class\n",
    "    for l in range(xtrainx1_index.shape[0]):\n",
    "        ytrain1[l,0] = 1 #label for one peak\n",
    "    \n",
    "    for m in range(Xtrainx2_filtered_arr.shape[0]):\n",
    "        ytrain2[m,1] = 1 #label for two peaks\n",
    "\n",
    "    for n in range(Xtrainx3_filtered_arr.shape[0]):\n",
    "        ytrain3[n,2] = 1 #label for three peaks\n",
    "\n",
    "    #concatenate the filtered training data and their corresponding labels\n",
    "    Xtrain = np.concatenate((xtrainx1_index, Xtrainx2_filtered_arr))\n",
    "    Xtrain = np.concatenate((Xtrain, Xtrainx3_filtered_arr))\n",
    "    Ytrain = np.concatenate((ytrain1, ytrain2))\n",
    "    Ytrain = np.concatenate((Ytrain, ytrain3))\n",
    "    \n",
    "    #shuffle the training data and labels\n",
    "    indices = np.arange(Xtrain.shape[0])\n",
    "    indices_shuffle = np.random.permutation(indices)\n",
    "    xtrain = Xtrain[indices_shuffle]\n",
    "    ytrain = Ytrain[indices_shuffle]\n",
    "    \n",
    "    #determine training, validation and test set sizes\n",
    "    Ntrain = xtrain.shape[0] - 2*int(xtrain.shape[0]*0.1)\n",
    "    Nval = int(xtrain.shape[0]*0.1)\n",
    "    Ntest = int(xtrain.shape[0]*0.1)\n",
    "    \n",
    "    #split the data into training, validation and test sets containing the Fourier coefficients\n",
    "    xtrainf, xval, xtest, ytrainf, yval, ytest = fouriertrainvaltest(xtrain, ytrain, Ntrain, Nval, Ntest)\n",
    "\n",
    "    #buld the model using the best hyperparameters\n",
    "    model = HyperModel().build(best_hps)\n",
    "    \n",
    "    #Fit the model on the training data with validation\n",
    "    history = model.fit(xtrainf[:,:-1], ytrainf, epochs = 1000, validation_data = (xval[:,:-1], yval), batch_size = best_hps.get('batch_size'), verbose=0)\n",
    "    \n",
    "    #evaluate the model on the training, validation and test sets and store the loss and classification accuracy\n",
    "    trainingloss[i], trainingacc[i] = model.evaluate(xtrainf[:,:-1], ytrainf)\n",
    "    valloss[i], valacc[i] = model.evaluate(xval[:,:-1], yval)\n",
    "    testloss[i], testacc[i] = model.evaluate(xtest[:,:-1], ytest)\n",
    "    \n",
    "    #save the accuracy and loss results to csv files\n",
    "    np.savetxt('traininglossvepsilon.csv', trainingloss, delimiter=',')\n",
    "    np.savetxt('trainingaccvepsilon.csv', trainingacc, delimiter=',')\n",
    "    np.savetxt('vallossvepsilon.csv', valloss, delimiter=',')\n",
    "    np.savetxt('valaccvepsilon.csv', valacc, delimiter=',')\n",
    "    np.savetxt('testlossvepsilon.csv', testloss, delimiter=',')\n",
    "    np.savetxt('testaccvepsilon.csv', testacc, delimiter=',')\n",
    " \n",
    "    #get correct and incorrect label predictions using the defined function\n",
    "    p1l1, p1l2, p1l3, p2l1, p2l2, p2l3, p3l1, p3l2, p3l3 = correctandincorrectlabels(xtest, ytest)\n",
    "    \n",
    "    #save the predictions for each category to csv files\n",
    "    \n",
    "    if len(p1l1)>0:\n",
    "        p1l1arr = np.concatenate(p1l1)\n",
    "        np.savetxt('Predictions/predictions1labelled1_epsilon{0}.csv'.format(epsilon[i]), p1l1arr, delimiter=',')\n",
    "        \n",
    "    if len(p1l2)>0:\n",
    "        p1l2arr = np.concatenate(p1l2)\n",
    "        np.savetxt('Predictions/predictions1labelled2_epsilon{0}.csv'.format(epsilon[i]), p1l2arr, delimiter=',')\n",
    "        \n",
    "    if len(p1l3)>0:\n",
    "        p1l3arr = np.concatenate(p1l3)\n",
    "        np.savetxt('Predictions/predictions1labelled3_epsilon{0}.csv'.format(epsilon[i]), p1l3arr, delimiter=',')\n",
    "        \n",
    "    if len(p2l1)>0:\n",
    "        p2l1arr = np.concatenate(p2l1)\n",
    "        np.savetxt('Predictions/predictions2labelled1_epsilon{0}.csv'.format(epsilon[i]), p2l1arr, delimiter=',')\n",
    "        \n",
    "    if len(p2l2)>0:\n",
    "        p2l2arr = np.concatenate(p2l2)\n",
    "        np.savetxt('Predictions/predictions2labelled2_epsilon{0}.csv'.format(epsilon[i]), p2l2arr, delimiter=',')\n",
    "\n",
    "    if len(p2l3)>0:\n",
    "        p2l3arr = np.concatenate(p2l3)\n",
    "        np.savetxt('Predictions/predictions2labelled3_epsilon{0}.csv'.format(epsilon[i]), p2l3arr, delimiter=',')\n",
    "        \n",
    "    if len(p3l1)>0:\n",
    "        p3l1arr = np.concatenate(p3l1)\n",
    "        np.savetxt('Predictions/predictions3labelled1_epsilon{0}.csv'.format(epsilon[i]), p3l1arr, delimiter=',')\n",
    "        \n",
    "    if len(p3l2)>0:\n",
    "        p3l2arr = np.concatenate(p3l2)\n",
    "        np.savetxt('Predictions/predictions3labelled2_epsilon{0}.csv'.format(epsilon[i]), p3l2arr, delimiter=',')\n",
    "        \n",
    "    if len(p3l3)>0:\n",
    "        p3l3arr = np.concatenate(p3l3)\n",
    "        np.savetxt('Predictions/predictions3labelled3_epsilon{0}.csv'.format(epsilon[i]), p3l3arr, delimiter=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
